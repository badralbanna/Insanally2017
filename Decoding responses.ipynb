{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import datetime, time\n",
    "import baysian_neural_decoding as bnd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipyparallel import Client\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run animal_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of active engines: 2\n"
     ]
    }
   ],
   "source": [
    "# Initializing engines\n",
    "rc = Client()\n",
    "dv = rc[:]\n",
    "dv.block = True\n",
    "dv.activate()\n",
    "print(\"Number of active engines: {0}\".format(len(dv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:0] \n",
      "/Users/badr/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/badr/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "[stderr:1] \n",
      "/Users/badr/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/badr/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Loading packages on engines\n",
    "%%px --block\n",
    "import sys\n",
    "# Add path to \n",
    "sys.path.append('YOUR_BASE_DIRECTORY/baysian_neural_decoding/')\n",
    "import numpy\n",
    "import baysian_neural_decoding as bnd\n",
    "from random import shuffle\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run animal_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def current_time(abbreviated = False):\n",
    "    if not abbreviated:\n",
    "        time_string = '%Y-%m-%d %H:%M:%S'\n",
    "    else:\n",
    "        time_string = '%Y%m%d-%H%M%S'\n",
    "    return(datetime.datetime.fromtimestamp(time.time()).strftime(time_string))\n",
    "\n",
    "def append_log(file_name, message, echo=False):\n",
    "    full_message = '[' + current_time() + '] ' + message + '\\n' \n",
    "    with open(file_name, \"a\") as f:\n",
    "        f.write(full_message)\n",
    "    if echo:\n",
    "        print(full_message)\n",
    "    pass\n",
    "\n",
    "def convert_data_format(s, a, np, data):\n",
    "    c = (s == 'T')*(a == 'NP')+(s == 'F')*(a == 'W')\n",
    "    \n",
    "    return({'all_stimulus': [ar['stimulus']['counts_summary'] for ar in data],\n",
    "            'all_action': [ar['choice']['counts_summary'] for ar in data],\n",
    "            'correct_stimulus': [ar['stimulus']['correct_counts_summary'] for ar in data],\n",
    "            'correct_action': [ar['choice']['correct_counts_summary'] for ar in data],\n",
    "            'incorrect_stimulus': [ar['stimulus']['incorrect_counts_summary'] for ar in data],\n",
    "            'incorrect_action': [ar['choice']['incorrect_counts_summary'] for ar in data],\n",
    "            'all_stimulus_probs': [ar['stimulus']['probs_summary'] for ar in data],\n",
    "            'all_action_probs': [ar['choice']['probs_summary'] for ar in data],\n",
    "            'correct_stimulus_probs': [ar['stimulus']['correct_probs_summary'] for ar in data], \n",
    "            'correct_action_probs': [ar['choice']['correct_probs_summary'] for ar in data],\n",
    "            'incorrect_stimulus_probs': [ar['stimulus']['incorrect_probs_summary'] for ar in data], \n",
    "            'incorrect_action_probs': [ar['choice']['incorrect_probs_summary'] for ar in data],\n",
    "            'stimulus_choices': [ar['stimulus']['counts'] for ar in data], \n",
    "            'action_choices': [ar['choice']['counts'] for ar in data],\n",
    "            'stimulus_probs': [ar['stimulus']['probs'] for ar in data],\n",
    "            'action_probs': [ar['choice']['probs'] for ar in data],\n",
    "            'stimulus_times': [ar['stimulus']['times'] for ar in data],\n",
    "            'action_times': [ar['choice']['times'] for ar in data],\n",
    "            'stimulus': s,\n",
    "            'action': a,\n",
    "            'nosepokes': np,\n",
    "            'correct' : c})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def animal_script(animal_info, neuron, log_file, case='whole_trial', spike_cutoff=3, **kwargs):\n",
    "        # Collecting the basic information\n",
    "        multiple = FLAGS['multiple']\n",
    "        append_log(log_file, \"  Loading data...\")\n",
    "        trial_duration = animal_info['trial_duration']\n",
    "        last_trial = animal_info['last_trial']\n",
    "        if not multiple:\n",
    "            neuron = [neuron]\n",
    "            \n",
    "        event_set, spike_set = bnd.load_events_spikes_script(neuron_num=neuron, **animal_info)\n",
    "        st0, s0, a0, np0, r0 = bnd.create_complete_table(event_set, spike_set, animal_info['variables'], \n",
    "                                                         trial_duration = trial_duration, \n",
    "                                                         pre_trial_duration = trial_duration)\n",
    "        \n",
    "        if not multiple:\n",
    "            r0 = r0[0]\n",
    "\n",
    "        if last_trial != None:\n",
    "            st0 = st0[:last_trial]\n",
    "            s0 = s0[:last_trial]\n",
    "            a0 = a0[:last_trial]\n",
    "            np0 = np0[:last_trial]\n",
    "            if multiple:\n",
    "                r0 = [r[:last_trial] for r in r0]\n",
    "            else:\n",
    "                r0 = r0[:last_trial]\n",
    "        else:\n",
    "            last_trial = len(s0)\n",
    "\n",
    "        # Filtering out low spike counts\n",
    "        cutoff_filter = bnd.spike_cutoff_script(r0, multiple=multiple, spike_cutoff=spike_cutoff)\n",
    "        st0 = numpy.array(st0)[cutoff_filter]\n",
    "        s0 = numpy.array(s0)[cutoff_filter]\n",
    "        a0 = numpy.array(a0)[cutoff_filter]\n",
    "        np0 = numpy.array(np0)[cutoff_filter]\n",
    "        if multiple:\n",
    "            r0 = [numpy.array(r)[cutoff_filter] for r in r0]\n",
    "        else:\n",
    "            r0 = numpy.array(r0)[cutoff_filter]\n",
    "\n",
    "        # Replacing withhold trials with average response time\n",
    "        avg_np = numpy.nanmean(np0)\n",
    "        trial_times = np0[:]\n",
    "        trial_times[numpy.isnan(trial_times)] = avg_np\n",
    "        num_trials = len(s0) \n",
    "        \n",
    "        # Creating spikes, inference times, and offsets on a case by case basis\n",
    "        if case is 'whole_trial':\n",
    "            # for standard analysis            \n",
    "            s_offset = numpy.matrix(num_trials*[0]).T\n",
    "            s_inf_times = numpy.matrix(zip(num_trials*[0], num_trials*[numpy.max(trial_times)]))\n",
    "            s_trial_times = numpy.matrix(zip(num_trials*[0], trial_times[:]))\n",
    "            c_offset = numpy.matrix(trial_times[:]).T\n",
    "            c_inf_times = numpy.matrix(zip(num_trials*[-numpy.max(trial_times)], num_trials*[0]))\n",
    "            c_trial_times = numpy.matrix(zip(-trial_times[:], num_trials*[0]))\n",
    "        elif case is 'whole_trial_pre':\n",
    "            # for standard analysis    \n",
    "            window = FLAGS['window']\n",
    "            s_offset = numpy.matrix(num_trials*[0]).T\n",
    "            s_inf_times = numpy.matrix(zip(num_trials*[- window], num_trials*[numpy.max(trial_times) + window]))\n",
    "            s_trial_times = numpy.matrix(zip(num_trials*[0], trial_times[:]))\n",
    "            c_offset = numpy.matrix(trial_times[:]).T\n",
    "            c_inf_times = numpy.matrix(zip(num_trials*[-numpy.max(trial_times) - window], num_trials*[window]))\n",
    "            c_trial_times = numpy.matrix(zip(-trial_times[:], num_trials*[0]))     \n",
    "        elif case is 'first_second':\n",
    "            # for standard analysis            \n",
    "            trial_length = FLAGS['trial_length']\n",
    "            s_offset = numpy.matrix(num_trials*[0]).T\n",
    "            s_inf_times = numpy.matrix(zip(num_trials*[0], num_trials*[trial_length]))\n",
    "            s_trial_times = numpy.matrix(zip(num_trials*[0], num_trials*[trial_length]))\n",
    "            c_offset = numpy.matrix(num_trials*[0]).T\n",
    "            c_inf_times = numpy.matrix(zip(num_trials*[0], num_trials*[trial_length]))\n",
    "            c_trial_times = numpy.matrix(zip(num_trials*[0], num_trials*[trial_length]))\n",
    "        elif case is 'last_second':\n",
    "            # for standard analysis            \n",
    "            trial_length = FLAGS['trial_length']\n",
    "            s_offset = numpy.matrix(trial_times[:]).T\n",
    "            s_inf_times = numpy.matrix(zip(num_trials*[- trial_length], num_trials*[0]))\n",
    "            s_trial_times = numpy.matrix(zip(num_trials*[- trial_length], num_trials*[0]))  \n",
    "            c_offset = numpy.matrix(trial_times[:]).T\n",
    "            c_inf_times = numpy.matrix(zip(num_trials*[- trial_length], num_trials*[0]))\n",
    "            c_trial_times = numpy.matrix(zip(num_trials*[- trial_length], num_trials*[0])) \n",
    "        elif case is 'first_second_pre':\n",
    "            # for standard analysis            \n",
    "            window = FLAGS['window']\n",
    "            trial_length = FLAGS['trial_length']\n",
    "            s_offset = numpy.matrix(num_trials*[0]).T\n",
    "            s_inf_times = numpy.matrix(zip(num_trials*[- window], num_trials*[trial_length + window]))\n",
    "            s_trial_times = numpy.matrix(zip(num_trials*[0], num_trials*[trial_length]))\n",
    "            c_offset = numpy.matrix(num_trials*[0]).T\n",
    "            c_inf_times = numpy.matrix(zip(num_trials*[- window], num_trials*[trial_length + window]))\n",
    "            c_trial_times = numpy.matrix(zip(num_trials*[0], num_trials*[trial_length]))\n",
    "        elif case is 'last_second_pre':\n",
    "            # for standard analysis            \n",
    "            window = FLAGS['window']\n",
    "            trial_length = FLAGS['trial_length']\n",
    "            s_offset = numpy.matrix(trial_times[:]).T\n",
    "            s_inf_times = numpy.matrix(zip(num_trials*[- trial_length - window], num_trials*[window]))\n",
    "            s_trial_times = numpy.matrix(zip(num_trials*[-trial_length], num_trials*[0]))  \n",
    "            c_offset = numpy.matrix(trial_times[:]).T\n",
    "            c_inf_times = numpy.matrix(zip(num_trials*[-trial_length - window], num_trials*[window]))\n",
    "            c_trial_times = numpy.matrix(zip(num_trials*[-trial_length], num_trials*[0]))              \n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # Converting the responses\n",
    "        append_log(log_file, \"  Converting responses...\")\n",
    "\n",
    "        max_time = numpy.max([s_inf_times + s_offset, s_trial_times + s_offset,  c_inf_times + c_offset,  c_trial_times + c_offset])\n",
    "        min_time = numpy.min([s_inf_times + s_offset, s_trial_times + s_offset,  c_inf_times + c_offset,  c_trial_times + c_offset])\n",
    "        if multiple:\n",
    "            spikes = [[ numpy.array(resp[(resp > min_time)*(resp < max_time)]) for resp in r ] for r in r0]\n",
    "        else:\n",
    "            spikes = [ numpy.array(resp[(resp > min_time)*(resp < max_time)]) for resp in r0 ]\n",
    "        \n",
    "        times = {\n",
    "            'total': (min_time, max_time),\n",
    "            'stimulus': [s_offset, s_inf_times, s_trial_times],\n",
    "            'choice': [c_offset, c_inf_times, c_trial_times]\n",
    "                }\n",
    "\n",
    "        variable_dict = {\n",
    "            'stimulus': [{'T':0, 'F':1}, s0], \n",
    "            'choice': [{'NP':0, 'W':1}, a0]\n",
    "                }\n",
    "\n",
    "        condition_variables = {\n",
    "            'correct': (s0 == 'T')*(a0 == 'NP')+(s0 == 'F')*(a0 == 'W'),\n",
    "            'incorrect': (s0 == 'T')*(a0 == 'W')+(s0 == 'F')*(a0 == 'NP')\n",
    "                }\n",
    "        \n",
    "        append_log(log_file, \"  Calculating parameters...\")\n",
    "        PARAMS = bnd.pre_script(variable_dict, spikes, times, RESP_FUNCTION, PRE_FUNCTION, condition_variables=condition_variables, num_folds=NUM_FOLDS, **FLAGS)\n",
    "        dv.push({'PARAMS': PARAMS})\n",
    "            \n",
    "        dv.push({'variable_dict' : variable_dict, \n",
    "                 'condition_variables' : condition_variables, \n",
    "                 'spikes' : spikes, \n",
    "                 'times' : times, \n",
    "                 'trial_duration' : trial_duration})\n",
    "                \n",
    "        # Running loop on engines\n",
    "        append_log(log_file, \"  Calculating predictions...\")\n",
    "        %px collection = [ bnd.main_script(variable_dict, spikes, times, RESP_FUNCTION, PROB_FUNCTION, condition_variables=condition_variables, num_folds=NUM_FOLDS, params=PARAMS, **FLAGS) for i in xrange(NUM_REPETITIONS) ]\n",
    "        \n",
    "        # Gathering data from engines\n",
    "        append_log(log_file, \"  Gathering predictions...\")\n",
    "        collection = dv.gather('collection', block = True)\n",
    "        return(s0, a0, np0, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on one Animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ANIMAL = 'PFC_12302014'\n",
    "NEURON = 1\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "NUM_REPETITIONS = 2\n",
    "SPIKE_CUTOFF = 3\n",
    "CASE = 'first_second'\n",
    "\n",
    "# Response function choses how to charicterize the spike trains (ISIs, first spike latency, etc)\n",
    "# See the modules included in the baysian_neural_decoding package for functions\n",
    "RESP_FUNCTION = bnd.calc_ISIs\n",
    "\n",
    "# Probability function determines how to model the responses\n",
    "PROB_FUNCTION = bnd.timed_prob\n",
    "\n",
    "# Pre-function determines any hyperparameters needed for the algorithm\n",
    "PRE_FUNCTION = bnd.set_bw\n",
    "\n",
    "# Additional flags\n",
    "FLAGS = {'multiple': False,\n",
    "         'use_false': False,\n",
    "         'use_PSTH': False,\n",
    "         'within_class': False,\n",
    "         'shuffle': False,\n",
    "         'prob_from_spikes': False, \n",
    "         'at_best': False, \n",
    "         'log': True, \n",
    "         'window':  1.0,\n",
    "         'step': .1,\n",
    "         'bin_size': .020,\n",
    "         'trial_length': .750,\n",
    "         'model': 'kde'}\n",
    "\n",
    "log_file = '../test.log'\n",
    "open(log_file, \"w\").close()\n",
    "\n",
    "append_log(log_file, \"Sending settings to engines.\")\n",
    "dv.push({'RESP_FUNCTION': RESP_FUNCTION,\n",
    "         'PROB_FUNCTION': PROB_FUNCTION,\n",
    "         'PRE_FUNCTION': PRE_FUNCTION,\n",
    "         'NUM_FOLDS': NUM_FOLDS,\n",
    "         'NUM_REPETITIONS': NUM_REPETITIONS,\n",
    "         'FLAGS': FLAGS})\n",
    "\n",
    "s, a, np, data = animal_script(ANIMALS[ANIMAL], NEURON, log_file, spike_cutoff=SPIKE_CUTOFF, case=CASE)\n",
    "new_data = convert_data_format(s, a, np, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 76.,  17.],\n",
       "        [ 67.,  38.]]), array([[ 76.,  17.],\n",
       "        [ 69.,  36.]]), array([[ 74.,  19.],\n",
       "        [ 68.,  37.]]), array([[ 75.,  18.],\n",
       "        [ 69.,  36.]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['all_stimulus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 48.07419247,  44.92580753],\n",
       "        [ 51.45156376,  53.54843624]]), array([[ 47.43892919,  45.56107081],\n",
       "        [ 51.39214266,  53.60785734]]), array([[ 46.90030863,  46.09969137],\n",
       "        [ 51.49863182,  53.50136818]]), array([[ 47.64025896,  45.35974104],\n",
       "        [ 51.60939895,  53.39060105]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['all_stimulus_probs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on All Animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-06-22 17:31:39] *** Starting new run ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "DIRECTORY = \"../results/ISI_in_time/\"\n",
    "NUM_REPETITIONS = 124 \n",
    "\n",
    "num_engines = len(dv)\n",
    "num_engine_reps = int(NUM_REPETITIONS / num_engines)\n",
    "multiple = FLAGS['multiple']\n",
    "\n",
    "log_file = DIRECTORY + '/run.log'\n",
    "if not os.path.exists(DIRECTORY):\n",
    "    os.mkdir(DIRECTORY)\n",
    "if not os.path.exists(log_file):\n",
    "    open(log_file, \"w\").close()\n",
    "\n",
    "append_log(log_file, '*** Starting new run ***', echo=True)\n",
    "append_log(log_file, \"Sending settings to engines.\")\n",
    "dv.push({'RESP_FUNCTION': RESP_FUNCTION,\n",
    "         'PROB_FUNCTION': PROB_FUNCTION,\n",
    "         'PRE_FUNCTION': PRE_FUNCTION,\n",
    "         'NUM_FOLDS': NUM_FOLDS, \n",
    "         'NUM_REPETITIONS': num_engine_reps,\n",
    "         'FLAGS': FLAGS})\n",
    "\n",
    "for animal in ANIMALS.keys():\n",
    "    if ANIMALS[animal]['include'] == True:\n",
    "        append_log(log_file, \"Starting animal {0}\".format(animal))\n",
    "        \n",
    "        if not multiple:\n",
    "            current_file = DIRECTORY+'/'+animal+'.pickle'\n",
    "            try:\n",
    "                with open(current_file, 'rb') as f:\n",
    "                    result_array = pickle.load(f)\n",
    "                append_log(log_file, \"  already have {0}...\".format(animal))\n",
    "            except:\n",
    "                result_array = {}\n",
    "        \n",
    "        # Picking what combinations\n",
    "        if multiple:\n",
    "            neurons = powerset(ANIMALS[animal]['choice_neurons'])\n",
    "        else:\n",
    "            neurons = ANIMALS[animal]['choice_neurons']\n",
    "        \n",
    "        for neuron in neurons:\n",
    "            if multiple and len(neuron) == 0:\n",
    "                continue\n",
    "            if multiple:\n",
    "                current_file = DIRECTORY + '/' + animal + '-' + str(neuron) + '.pickle'\n",
    "                try:\n",
    "                    with open(current_file, 'rb') as f:\n",
    "                        result_array = pickle.load(f)\n",
    "                    append_log(log_file, \"  already have {0}, {1}...\".format(animal, neuron))\n",
    "                except:\n",
    "                    result_array = {}\n",
    "            if result_array.has_key(neuron):\n",
    "                append_log(log_file, \"  ...skipping neuron {0}.\".format(neuron))\n",
    "                continue\n",
    "            else:\n",
    "                append_log(log_file, \"Animal {0}, neuron {1}\".format(animal, neuron))\n",
    "            try:\n",
    "                result_array[neuron] = convert_data_format(*animal_script(ANIMALS[animal], neuron, log_file, spike_cutoff=SPIKE_CUTOFF, case=CASE))                \n",
    "            except KeyboardInterrupt:\n",
    "                append_log(log_file, \"KEYBOARD INTERRUPT!\")\n",
    "                raise\n",
    "            except:\n",
    "                append_log(log_file, \"  Problem with animal {0}, neuron {1}\".format(animal, neuron))\n",
    "                #raise\n",
    "            finally:\n",
    "                # Saving the output\n",
    "                append_log(log_file, \"  Saving output...\")\n",
    "                with open(current_file, 'wb') as f:\n",
    "                    pickle.dump(result_array, f)\n",
    "open(DIRECTORY + \"/run complete\", \"wb\").close()\n",
    "append_log(log_file, \"Run complete.\", echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
